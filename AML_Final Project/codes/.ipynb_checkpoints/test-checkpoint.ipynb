{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def get_info(x):\n",
    "    return [i.split(\":\")[-1] for i in x.split(\" \")]\n",
    "def get_speed(x):\n",
    "    return np.array([i.split(\",\")[0] for i in x],dtype='float16')\n",
    "def get_eta(x):\n",
    "    return np.array([i.split(\",\")[1] for i in x],dtype=\"float16\")\n",
    "def get_state(x):\n",
    "    return np.array([i.split(\",\")[2] for i in x])\n",
    "def get_cnt(x):\n",
    "    return np.array([i.split(\",\")[3] for i in x],dtype=\"int16\")\n",
    "\n",
    "\n",
    "def get_feature(input_file_path_his, input_file_path_attr,input_file_path_topo, mode):\n",
    "    # his\n",
    "    df = pd.read_csv(input_file_path_his, sep=\";\", header=None)\n",
    "    df[\"link\"] = df[0].apply(lambda x: x.split(\" \")[0]).astype(int)\n",
    "    df[\"label\"] = df[0].apply(lambda x: x.split(\" \") [1]).astype(int)\n",
    "    df[\"current_slice_id\"] = df[0].apply(lambda x: x.split(\" \")[2]).astype(int)\n",
    "    df[\"future_slice_id\"] = df[0].apply(lambda x: x.split(\" \")[3]).astype(int)\n",
    "    df[\"time_diff\"] = df[\"future_slice_id\"] - df[\"current_slice_id\"]\n",
    "    df = df.drop([0], axis=1)\n",
    "\n",
    "    if mode == \"is_train\":\n",
    "        df[\"label\"] = df[\"label\"].map(lambda x: 3 if x >= 3 else x)\n",
    "        df['label'] -= 1\n",
    "    else:\n",
    "        df = df.drop([\"label\"], axis=1)\n",
    "\n",
    "    df[\"current_state_last\"] = df[1].apply(lambda x: x.split(\" \")[-1].split(\":\")[-1])\n",
    "        # 路况速度,eta速度,路况状态,参与路况计算的车辆数\n",
    "    df[\"current_speed\"] = df[\"current_state_last\"].apply(lambda x: x.split(\",\")[0])\n",
    "    df[\"current_eat_speed\"] = df[\"current_state_last\"].apply(lambda x: x.split(\",\")[1])\n",
    "    df[\"current_state\"] = df[\"current_state_last\"].apply(lambda x: x.split(\",\")[2])\n",
    "    df[\"current_count\"] = df[\"current_state_last\"].apply(lambda x: x.split(\",\")[3])\n",
    "    df = df.drop([\"current_state_last\"], axis=1)\n",
    "    for i in tqdm(range(1, 6, 1)):\n",
    "        flag = f\"his_{(6-i)*7}\"\n",
    "        df[\"history_info\"] = df[i].apply(get_info)\n",
    "\n",
    "        # speed\n",
    "        df[\"his_speed\"] = df[\"history_info\"].apply(get_speed)\n",
    "        df[f'{flag}_speed_mean'] = df[\"his_speed\"].apply(lambda x: x.mean())\n",
    "\n",
    "        # eta\n",
    "        df[\"his_eta\"] = df[\"history_info\"].apply(get_eta)\n",
    "        df[f\"{flag}_eta_mean\"] = df[\"his_eta\"].apply(lambda x: x.mean())\n",
    "\n",
    "\n",
    "        # state\n",
    "        df[\"his_state\"] = df[\"history_info\"].apply(get_state)\n",
    "        df[f\"{flag}_state_max\"] = df[\"his_state\"].apply(lambda x: Counter(x).most_common()[0][0])\n",
    "        df[f\"{flag}_state_min\"] = df[\"his_state\"].apply(lambda x: Counter(x).most_common()[-1][0])\n",
    "\n",
    "        # cnt\n",
    "        df[\"his_cnt\"] = df[\"history_info\"].apply(get_cnt)\n",
    "        df[f\"{flag}_cnt_mean\"] = df[\"his_cnt\"].apply(lambda x: x.mean())\n",
    "        df = df.drop([i, \"history_info\", \"his_speed\", \"his_eta\", \"his_state\", \"his_cnt\"], axis=1)\n",
    "        # break\n",
    "\n",
    "    df2 = pd.read_csv(input_file_path_attr, sep='\\t',\n",
    "                       names=['link', 'length', 'direction', 'path_class', 'speed_class',\n",
    "                              'LaneNum', 'speed_limit', 'level', 'width'], header=None)\n",
    "    df = df.merge(df2, on='link', how='left')\n",
    "\n",
    "    if mode ==\"is_train\":\n",
    "        output_file_path =f\"./data/{mode}_{input_file_path_his.split('/')[-1].split('.')[0]}\" +\".csv\"\n",
    "        df.to_csv(output_file_path,index =False,mode='w', header=True)\n",
    "\n",
    "    else:\n",
    "        output_file_path=f\"./data/{input_file_path_his.split('/')[-1].split('.')[0]}\" +\".csv\"\n",
    "        df.to_csv(output_file_path,index = False,mode='w', header=True)\n",
    "    # print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 23:29:29.426964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:26<00:00, 41.25s/it]\n",
      "100%|██████████| 5/5 [03:24<00:00, 40.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 23:37:13.921012\n"
     ]
    }
   ],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    print(datetime.datetime.now())\n",
    "    #训练集\n",
    "    get_feature(input_file_path_his=\"data/traffic/20190701.txt\",\\\n",
    "                input_file_path_attr=\"data/attr.txt\",\\\n",
    "                input_file_path_topo=\"data/topo.txt\",mode=\"is_train\")\n",
    "    #测试集\n",
    "    get_feature(input_file_path_his=\"data/test/test.txt\",\\\n",
    "                input_file_path_attr=\"data/attr.txt\",\\\n",
    "                input_file_path_topo=\"data/topo.txt\",mode=\"is_test\")\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Version: python 3.7.9  tensorflow 2.0.0 keras 2.3.1\n",
    "# Name:Model DNN\n",
    "# =======================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from keras_applications import vgg16\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from keras import losses\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def Dnn_Model(train=None,label=None, test=None, use_features=None,categorical_feats=None, n_class=3):\n",
    "    input_length=train[use_features].shape[0]\n",
    "    input_dim=train[use_features].shape[1]\n",
    "    \n",
    "    Y= np_utils.to_categorical(train[label],num_classes=3)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(train[use_features], Y,\n",
    "                                                        train_size=0.7, test_size=0.3, random_state=0)\n",
    "    # 2. 定义模型\n",
    "    init = K.initializers.glorot_uniform(seed=1)\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Dense(units=5, input_shape=(input_dim,), kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=6, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=n_class, kernel_initializer=init, activation='softmax'))\n",
    "    # rmsprop可以自定义,也可以使用默认值\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"rmsprop\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    b_size=128\n",
    "    max_epochs=10\n",
    "    model.fit(train_x, train_y, batch_size=b_size, epochs=max_epochs, shuffle=True,\n",
    "              validation_data=(test_x, test_y),verbose=1)\n",
    "    loss_and_metrics = model.evaluate(test_x, test_y, batch_size=128)\n",
    "    print(loss_and_metrics)\n",
    "\n",
    "    predictions = model.predict(test[use_features])\n",
    "\n",
    "    test[\"label\"] = np.argmax(predictions, axis=1) + 1\n",
    "    return test[[\"link\", 'current_slice_id', 'future_slice_id', \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 23:40:38.098227\n",
      "WARNING:tensorflow:From /Users/scarlett/opt/anaconda3/envs/zopencv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 352489 samples, validate on 151067 samples\n",
      "Epoch 1/10\n",
      "352489/352489 [==============================] - 7s 21us/step - loss: 0.4495 - accuracy: 0.8415 - val_loss: 0.3948 - val_accuracy: 0.8570\n",
      "Epoch 2/10\n",
      "352489/352489 [==============================] - 7s 19us/step - loss: 0.3890 - accuracy: 0.8557 - val_loss: 0.3871 - val_accuracy: 0.8547\n",
      "Epoch 3/10\n",
      "352489/352489 [==============================] - 6s 18us/step - loss: 0.3839 - accuracy: 0.8577 - val_loss: 0.3797 - val_accuracy: 0.8585\n",
      "Epoch 4/10\n",
      "352489/352489 [==============================] - 6s 17us/step - loss: 0.3828 - accuracy: 0.8583 - val_loss: 0.3774 - val_accuracy: 0.8601\n",
      "Epoch 5/10\n",
      "352489/352489 [==============================] - 6s 16us/step - loss: 0.3823 - accuracy: 0.8587 - val_loss: 0.3824 - val_accuracy: 0.8582\n",
      "Epoch 6/10\n",
      "352489/352489 [==============================] - 6s 18us/step - loss: 0.3820 - accuracy: 0.8593 - val_loss: 0.3865 - val_accuracy: 0.8566\n",
      "Epoch 7/10\n",
      "352489/352489 [==============================] - 6s 17us/step - loss: 0.3821 - accuracy: 0.8591 - val_loss: 0.3815 - val_accuracy: 0.8590\n",
      "Epoch 8/10\n",
      "352489/352489 [==============================] - 6s 16us/step - loss: 0.3823 - accuracy: 0.8590 - val_loss: 0.3785 - val_accuracy: 0.8601\n",
      "Epoch 9/10\n",
      "352489/352489 [==============================] - 6s 17us/step - loss: 0.3824 - accuracy: 0.8592 - val_loss: 0.3809 - val_accuracy: 0.8588\n",
      "Epoch 10/10\n",
      "352489/352489 [==============================] - 6s 17us/step - loss: 0.3816 - accuracy: 0.8595 - val_loss: 0.3766 - val_accuracy: 0.8608\n",
      "151067/151067 [==============================] - 1s 6us/step\n",
      "[0.376646004336733, 0.8608365654945374]\n",
      "2020-12-13 23:41:52.307705\n"
     ]
    }
   ],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    train = pd.read_csv('data/is_train_20190701.csv')\n",
    "    test = pd.read_csv(\"data/test.csv\")\n",
    "    # print(train.dtypes)\n",
    "    # print(train[\"link_id_length\"].unique())\n",
    "    \n",
    "    del_feature = ['link','label']\n",
    "    use_features = [i for i in train.columns if i not in del_feature]\n",
    "    category = [\"direction\",\"pathclass\",\"speedclass\",\"LaneNum\",\"level\"]\n",
    "    print(datetime.datetime.now())\n",
    "    submit =Dnn_Model(train=train,label=\"label\", test=test, use_features=use_features,\n",
    "                      categorical_feats=None, n_class=3)\n",
    "    submit.to_csv('submit.csv', index=False, encoding='utf8')\n",
    "\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN model\n",
    "\n",
    "def DNN_Model(dataset=None,label=None,use_features=None,categorical_feats=None, n_class=3):\n",
    "    input_length = dataset[use_features].shape[0]\n",
    "    input_dim = dataset[use_features].shape[1]\n",
    "    \n",
    "    Y = np_utils.to_categorical(dataset[label],num_classes=3)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataset[use_features], Y,\n",
    "                                                        train_size=0.7, test_size=0.3, random_state=0)\n",
    "    \n",
    "    # define the model \n",
    "    init = K.initializers.glorot_uniform(seed=1)\n",
    "    simple_adam = K.optimizers.Adam()\n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.layers.Dense(units=5, input_shape=(input_dim,), kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=6, kernel_initializer=init, activation='relu'))\n",
    "    model.add(K.layers.Dense(units=n_class, kernel_initializer=init, activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=simple_adam,\n",
    "                  #optimizer=\"rmsprop\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    # train the model\n",
    "    print (\"Start training \\n\")\n",
    "        \n",
    "    weights = {0:1,1:1,2:2}\n",
    "    folds = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train), start=1):\n",
    "        print('----------------------\\n'\n",
    "              'fold '+str(n_fold)+'开始了\\n'+'本轮共有'+str(len(train_idx))+'条数据参与训练')\n",
    "        train_x2, train_y2 = x_train[train_idx], y_train[train_idx]\n",
    "        valid_x2, valid_y2 = x_train[valid_idx], y_train[valid_idx]\n",
    "        model.fit(train_x2, train_y2,batch_size = 100, epochs=3, validation_data=(valid_x2,valid_y2),\n",
    "                  validation_freq=1,class_weight = weights)\n",
    "        \n",
    "        y_preds = model.predict(x_test[use_features])\n",
    "        y_preds = np.argmax(y_preds, axis=1).reshape(-1)\n",
    "        f1_scores = f1_score(y_test, y_preds, average=None) \n",
    "        final_f1 = f1_scores[0] * 0.2 + f1_scores[1] * 0.2 + f1_scores[2] * 0.6\n",
    "    \n",
    "    print (\"Training completed \\n\")\n",
    "\n",
    "    return final_f1,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model\n",
    "\n",
    "dataset = pd.read_csv('input/dataset_merged_2d.csv')[:10000]\n",
    "\n",
    "del_feature = ['link','label']\n",
    "use_features = [i for i in dataset.columns if i not in del_feature]\n",
    "category = [\"direction\",\"pathclass\",\"speedclass\",\"LaneNum\",\"level\"]\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "DNN = DNN_Model(dataset=dataset,label=\"label\",use_features=use_features,\n",
    "                  categorical_feats=None, n_class=3)\n",
    "final_f1 = DNN[0]\n",
    "history = DNN[1]\n",
    "\n",
    "print(\"Final F1 on test set = \", final_f1)\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Scarlett)",
   "language": "python",
   "name": "scarlett"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
