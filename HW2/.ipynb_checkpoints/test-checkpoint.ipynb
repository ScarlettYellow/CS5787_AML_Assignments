{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"input/split_train.csv\")\n",
    "x_train=train.drop('target',axis=1)#dataframe with index\n",
    "y_train=train.target\n",
    "\n",
    "test=pd.read_csv(\"input/dev.csv\")\n",
    "x_test=test.drop('target',axis=1)#dataframe with index\n",
    "y_test=test.target\n",
    "\n",
    "tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True) # word segmentation\n",
    "stemmer=PorterStemmer() # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre=[]\n",
    "test_pre=[]\n",
    "def preprocess(t,kpc):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    tee=url.sub(r'',t)\n",
    "    tee=re.sub('[^a-zA-Z]',\" \",tee)\n",
    "    tee=tee.lower()\n",
    "    res=tweettoken.tokenize(tee)\n",
    "    for i in res:\n",
    "        if i in stopwords.words('english'):\n",
    "            res.remove(i)\n",
    "    rest=[]\n",
    "    for k in res:\n",
    "        rest.append(stemmer.stem(k))\n",
    "    ret=\" \".join(rest)\n",
    "    if kpc==1:\n",
    "        train_pre.append(ret)\n",
    "    elif kpc==0:\n",
    "        test_pre.append(ret)\n",
    "\n",
    "def splitpro(t,q,m):\n",
    "         for j in range(q):\n",
    "                 preprocess(t[\"text\"].iloc[j],m)\n",
    "                 \n",
    "splitpro(x_train,5329,1)\n",
    "splitpro(x_test,2284,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "['aa', 'aba', 'abandon', 'abbswinston', 'abc', 'abcnew', 'abil', 'abl', 'ablaz', 'about', 'absolut', 'abstorm', 'abus', 'accept', 'access', 'accid', 'accident', 'accionempresa', 'accord', 'account', 'accus', 'achiev', 'acid', 'acr', 'across', 'act', 'action', 'activ', 'actual', 'ad', 'add', 'address', 'admit', 'adopt', 'adult', 'advanc', 'advisori', 'af', 'affect', 'affili', 'afghan', 'afghanistan', 'afp', 'afraid', 'africa', 'after', 'afternoon', 'aftershock', 'ag', 'again']\n",
      "[('ash', 169), ('australia', 192), ('collaps', 532), ('trent', 2721), ('bridg', 355), ('among', 99), ('worst', 2941), ('histori', 1240), ('england', 859), ('bundl', 381), ('great', 1140), ('michigan', 1652), ('techniqu', 2592), ('camp', 408), ('thank', 2617), ('hail', 1171), ('cnn', 523), ('tennesse', 2602), ('movi', 1714), ('theater', 2621), ('shoot', 2346), ('suspect', 2555), ('kill', 1428), ('polic', 1984), ('still', 2494), ('riot', 2205), ('coupl', 594), ('hour', 1264), ('left', 1483), ('to', 2665), ('up', 2788), ('class', 503), ('crack', 601), ('the', 2620), ('path', 1917), ('thi', 2631), ('morn', 1700), ('beach', 245), ('run', 2238), ('surfac', 2549), ('wound', 2945), ('right', 2203), ('knee', 1438), ('expert', 914), ('franc', 1042), ('begin', 262), ('examin', 897), ('airplan', 64), ('debri', 667), ('found', 1035)]\n",
      "[ 3  8  8 ...  6  6 24]\n",
      "(5329, 2991)\n",
      "total number of features in trainset: 2991\n"
     ]
    }
   ],
   "source": [
    "# d-1: use bag of words model on train set.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3,\n",
    "                             binary=True) #instantiate the CountVectorizer class, set M=3\n",
    "train_corpus = train_pre\n",
    "\n",
    "train_bow_vectors = vectorizer.fit_transform(train_corpus) #use fit() to create index, transform each document into a word frequency vector\n",
    "print(type(train_bow_vectors)) #type: sparse vector\n",
    "#print (train_bow_vectors)\n",
    "\n",
    "train_bow_vectors = train_bow_vectors.toarray()\n",
    "print (train_bow_vectors)\n",
    "print(type(train_bow_vectors)) #type: np array\n",
    "\n",
    "train_bow_voca_list = vectorizer.get_feature_names() #generate corpus into a vocabulary list\n",
    "train_bow_voca_dic = vectorizer.vocabulary_\n",
    "print(train_bow_voca_list[:50])\n",
    "print(list(train_bow_voca_dic.items())[:50])\n",
    "#print('vocabulary list of trainset:', train_bow_voca_list)\n",
    "#print( 'vocabulary dic of trainset:', train_bow_voca_dic)\n",
    "\n",
    "print(train_bow_vectors.sum(axis=0)) #count each word' frequency in the corpus\n",
    "\n",
    "X1 = train_bow_vectors.shape\n",
    "print (X1)\n",
    "train_bow_feature_num = X1[1] #vector length/number of features\n",
    "print ('total number of features in trainset:', train_bow_feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "['aa', 'aba', 'abandon', 'abbswinston', 'abc', 'abcnew', 'abil', 'abl', 'ablaz', 'about', 'absolut', 'abstorm', 'abus', 'accept', 'access', 'accid', 'accident', 'accionempresa', 'accord', 'account', 'accus', 'achiev', 'acid', 'acr', 'across', 'act', 'action', 'activ', 'actual', 'ad', 'add', 'address', 'admit', 'adopt', 'adult', 'advanc', 'advisori', 'af', 'affect', 'affili', 'afghan', 'afghanistan', 'afp', 'afraid', 'africa', 'after', 'afternoon', 'aftershock', 'ag', 'again']\n",
      "[('ash', 169), ('australia', 192), ('collaps', 532), ('trent', 2721), ('bridg', 355), ('among', 99), ('worst', 2941), ('histori', 1240), ('england', 859), ('bundl', 381), ('great', 1140), ('michigan', 1652), ('techniqu', 2592), ('camp', 408), ('thank', 2617), ('hail', 1171), ('cnn', 523), ('tennesse', 2602), ('movi', 1714), ('theater', 2621), ('shoot', 2346), ('suspect', 2555), ('kill', 1428), ('polic', 1984), ('still', 2494), ('riot', 2205), ('coupl', 594), ('hour', 1264), ('left', 1483), ('to', 2665), ('up', 2788), ('class', 503), ('crack', 601), ('the', 2620), ('path', 1917), ('thi', 2631), ('morn', 1700), ('beach', 245), ('run', 2238), ('surfac', 2549), ('wound', 2945), ('right', 2203), ('knee', 1438), ('expert', 914), ('franc', 1042), ('begin', 262), ('examin', 897), ('airplan', 64), ('debri', 667), ('found', 1035)]\n",
      "[ 0  6  6 ...  0  0 11]\n",
      "(2284, 2991)\n",
      "total number of features in dev set: 2991\n"
     ]
    }
   ],
   "source": [
    "# d-1: use bag of words model on dev set.\n",
    "\n",
    "test_corpus = test_pre\n",
    "\n",
    "test_bow_vectors = vectorizer.transform(test_corpus) #use the same set of tokens as trainset, transform each document into a word frequency vector\n",
    "print(type(test_bow_vectors)) #type: sparse vector\n",
    "#print (test_bow_vectors)\n",
    "\n",
    "test_bow_vectors = test_bow_vectors.toarray()\n",
    "print (test_bow_vectors)\n",
    "print(type(test_bow_vectors)) #type: np array\n",
    "\n",
    "test_bow_voca_list = vectorizer.get_feature_names() #generate corpus into a vocabulary list\n",
    "test_bow_voca_dic = vectorizer.vocabulary_\n",
    "print(test_bow_voca_list[:50])\n",
    "print(list(test_bow_voca_dic.items())[:50])\n",
    "#print('vocabulary list of dev set:', test_bow_voca_list)\n",
    "#print( 'vocabulary dic of dev set:', test_bow_voca_dic)\n",
    "\n",
    "print(test_bow_vectors.sum(axis=0)) #count each word' frequency in the corpus\n",
    "\n",
    "X2 = test_bow_vectors.shape\n",
    "print (X2)\n",
    "test_bow_feature_num = X2[1] #vector length/number of features\n",
    "print ('total number of features in dev set:', test_bow_feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-1: implement the BernoulliNB Naive Bayes classifier, without using any existing machine learning libraries.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Bernoulli_NaiveBayes:\n",
    "\n",
    "    def __init__(self):   \n",
    "        self.alpha = 1 # set smoothing factor=1(Laplace Smoothing), to avoid zero probability problems  \n",
    "\n",
    "    def _cal_prior_prob_log(self, y, classes): # calculate the logarithm of prior probability of each class, P(y=c_k)\n",
    "        self.classes = np.unique(y)\n",
    "        class_num = len(self.classes) #count the number of possible types of y\n",
    "        sample_num = len(y)\n",
    "        \n",
    "        c_num = np.count_nonzero(y == classes[:, None], axis=1) #count sample amount of each class\n",
    "        prior_prob = (c_num + self.alpha) / (sample_num + class_num * self.alpha) #calculate prior probabilities(add smoothing correction)\n",
    "        prior_prob_log = np.log(prior_prob) #calculate logarithm\n",
    "        \n",
    "        return prior_prob_log\n",
    "    \n",
    "    def _cal_condi_prob_log(self, X, y, classes): #calculate the logarithm of all conditional probabilities P(x^(j)|y=c_k)\n",
    "        \n",
    "        n = (X.shape)[1]\n",
    "        K = len(classes)\n",
    "        \n",
    "        #create an empty multidimensional array\n",
    "        #prob_log: logarithmic matrix of two conditional probabilities\n",
    "        condi_prob_log = np.empty((2, K, n)) \n",
    "        \n",
    "        for k, c in enumerate(classes):\n",
    "            X_c = X[np.equal(y, c)] #acquire all samples of class c_k\n",
    "            total_num = len(X_c)\n",
    "            num_f1 = np.count_nonzero(X_c, axis=0) #count the number of samples of which feature value is 1\n",
    "            condi_prob_f1 = (num_f1 + self.alpha) / (total_num + self.alpha * 2) #calculate conditional probability P(x^(j)=1|y=c_k)\n",
    "            \n",
    "            #calculate and store logarithm into matrix\n",
    "            #prob_log[0]: store all values of log(P(x^(j)=0|y=c_k))\n",
    "            #prob_log[1]: store all values of log(P(x^(j)=1|y=c_k))\n",
    "            condi_prob_log[0, k] = np.log(1 - condi_prob_f1) \n",
    "            condi_prob_log[1, k] = np.log(condi_prob_f1) \n",
    "            \n",
    "        return condi_prob_log\n",
    "   \n",
    "    def train(self, x_train, y_train): #train the model\n",
    "        self.classes = np.unique(y_train) #acquire all classes  \n",
    "        self.prior_prob_log = self._cal_prior_prob_log(y_train, self.classes) #calculate and store the logarithm of all prior probabilities\n",
    "        self.condi_prob_log = self._cal_condi_prob_log(x_train, y_train, self.classes) #calculate and store the logarithm of all conditional probabilities\n",
    "\n",
    "    def _predict_single_sample(self, x): #predict the label of single sample\n",
    "\n",
    "        K = len(self.classes)\n",
    "        po_prob_log = np.empty(K) #create an empty multidimensional array\n",
    "        \n",
    "        index_f1 = x == 1 #acquire index of feature value=1 \n",
    "        index_f0 = ~index_f1 #acquire index of feature value=0\n",
    "\n",
    "        for k in range(K): #iterate each class\n",
    "            #calculate the logarithm of the numerator of the posterior probability\n",
    "            po_prob_log[k] = self.prior_prob_log[k] \\\n",
    "                                + np.sum(self.condi_prob_log[0, k][index_f0]) \\\n",
    "                                + np.sum(self.condi_prob_log[1, k][index_f1])\n",
    "\n",
    "        label = np.argmax(po_prob_log) #get the class with the highest posterior probability\n",
    "        return label\n",
    "\n",
    "    def predict(self, X): #predict samples (include single sample)\n",
    "        \n",
    "        if X.ndim == 1: #if only predict single sample (the dimension of the array = 1), invoke _predict_single_sample()\n",
    "            return self._predict_single_sample(X) \n",
    "        else:\n",
    "            #if predict multiple samples, loop call _predict_single_sample() and return a list of the predicted results \n",
    "            labels = []\n",
    "            for j in range(X.shape[0]):\n",
    "                label = self._predict_single_sample(X[j])\n",
    "                labels.append(label)\n",
    "            return labels\n",
    "        \n",
    "    def f1_score(self, true,predict):\n",
    "        \n",
    "        num = len(true)#确定有几组\n",
    "        (TP, FP, TN, FN) = ([0] * num for i in range(4))#赋初值\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-1: implement the BernoulliNB Naive Bayes classifier, without using any existing machine learning libraries.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Bernoulli_NaiveBayes:\n",
    "\n",
    "    def __init__(self):   \n",
    "        self.alpha = 1 # set smoothing factor=1(Laplace Smoothing), to avoid zero probability problems  \n",
    "\n",
    "    def _cal_prior_prob_log(self, y, classes): # calculate the logarithm of prior probability of each class, P(y=c_k)\n",
    "        self.classes = np.unique(y)\n",
    "        class_num = len(self.classes) #count the number of possible types of y\n",
    "        sample_num = len(y)\n",
    "        \n",
    "        c_num = np.count_nonzero(y == classes[:, None], axis=1) #count sample amount of each class\n",
    "        prior_prob = (c_num + self.alpha) / (sample_num + class_num * self.alpha) #calculate prior probabilities(add smoothing correction)\n",
    "        prior_prob_log = np.log(prior_prob) #calculate logarithm\n",
    "        \n",
    "        return prior_prob_log\n",
    "    \n",
    "    def _cal_condi_prob_log(self, X, y, classes): #calculate the logarithm of all conditional probabilities P(x^(j)|y=c_k)\n",
    "        \n",
    "        n = (X.shape)[1]\n",
    "        K = len(classes)\n",
    "        \n",
    "        #create an empty multidimensional array\n",
    "        #prob_log: logarithmic matrix of two conditional probabilities\n",
    "        condi_prob_log = np.empty((2, K, n)) \n",
    "        \n",
    "        for k, c in enumerate(classes):\n",
    "            X_c = X[np.equal(y, c)] #acquire all samples of class c_k\n",
    "            total_num = len(X_c)\n",
    "            num_f1 = np.count_nonzero(X_c, axis=0) #count the number of samples of which feature value is 1\n",
    "            condi_prob_f1 = (num_f1 + self.alpha) / (total_num + self.alpha * 2) #calculate conditional probability P(x^(j)=1|y=c_k)\n",
    "            \n",
    "            #calculate and store logarithm into matrix\n",
    "            #prob_log[0]: store all values of log(P(x^(j)=0|y=c_k))\n",
    "            #prob_log[1]: store all values of log(P(x^(j)=1|y=c_k))\n",
    "            condi_prob_log[0, k] = np.log(1 - condi_prob_f1) \n",
    "            condi_prob_log[1, k] = np.log(condi_prob_f1) \n",
    "            \n",
    "        return condi_prob_log\n",
    "   \n",
    "    def train(self, x_train, y_train): #train the model\n",
    "        self.classes = np.unique(y_train) #acquire all classes  \n",
    "        self.prior_prob_log = self._cal_prior_prob_log(y_train, self.classes) #calculate and store the logarithm of all prior probabilities\n",
    "        self.condi_prob_log = self._cal_condi_prob_log(x_train, y_train, self.classes) #calculate and store the logarithm of all conditional probabilities\n",
    "\n",
    "    def _predict_single_sample(self, x): #predict the label of single sample\n",
    "\n",
    "        K = len(self.classes)\n",
    "        po_prob_log = np.empty(K) #create an empty multidimensional array\n",
    "        \n",
    "        index_f1 = x == 1 #acquire index of feature value=1 \n",
    "        index_f0 = ~index_f1 #acquire index of feature value=0\n",
    "\n",
    "        for k in range(K): #iterate each class\n",
    "            #calculate the logarithm of the numerator of the posterior probability\n",
    "            po_prob_log[k] = self.prior_prob_log[k] \\\n",
    "                                + np.sum(self.condi_prob_log[0, k][index_f0]) \\\n",
    "                                + np.sum(self.condi_prob_log[1, k][index_f1])\n",
    "\n",
    "        label = np.argmax(po_prob_log) #get the class with the highest posterior probability\n",
    "        return label\n",
    "\n",
    "    def predict(self, X): #predict samples (include single sample)\n",
    "        \n",
    "        if X.ndim == 1: #if only predict single sample (the dimension of the array = 1), invoke _predict_single_sample()\n",
    "            return self._predict_single_sample(X) \n",
    "        else:\n",
    "            #if predict multiple samples, loop call _predict_single_sample() and return a list of the predicted results \n",
    "            labels = []\n",
    "            for j in range(X.shape[0]):\n",
    "                label = self._predict_single_sample(X[j])\n",
    "                labels.append(label)\n",
    "            return labels\n",
    "        \n",
    "    def f1_score(self, true,predict):\n",
    "        \n",
    "        num = len(true)#确定有几组\n",
    "        (TP, FP, TN, FN) = ([0] * num for i in range(4))#赋初值\n",
    "        \n",
    "        #idx, logpyx, dev_label = nb_predictions(dev_data, dev_label, psis, phis)\n",
    "\n",
    "        for m in range(0,len(true)):\n",
    "            if(len(true[m]) != len(predict[m])):#样本数都不等，显然是有错误的\n",
    "                print (\"真实结果与预测结果样本数不一致。\")\n",
    "            else:\n",
    "                for i in range(0,len(true[m])):#对每一组数据分别计数\n",
    "                    if   (predict[m][i] == 1) and ((true[m][i] == 1)):    TP[m] += 1.0\n",
    "                    elif (predict[m][i] == 1) and ((true[m][i] == 0)):    FP[m] += 1.0\n",
    "                    elif (predict[m][i] == 0) and ((true[m][i] == 1)):    FN[m] += 1.0\n",
    "                    elif (predict[m][i] == 0) and ((true[m][i] == 0)):    TN[m] += 1.0\n",
    "\n",
    " \n",
    "        # macro度量，先求每一组的评价指标，再求均值\n",
    "        (accuracy_macro, \\\n",
    "         precision1_macro, precision0_macro, \\\n",
    "         recall1_macro, recall0_macro, \\\n",
    "         F1_score1_macro,F1_score0_macro) = \\\n",
    "         ([0] * num for i in range(7))\n",
    " \n",
    "        for m in range(0,num):\n",
    "\n",
    "            accuracy_macro[m]    = (TP[m] + TN[m]) / (TP[m] + FP[m] + FN[m] +TN[m])\n",
    "\n",
    "            if (TP[m] + FP[m] == 0) : precision1_macro[m] = 0#预防一些分母为0的情况\n",
    "            else :precision1_macro[m] = TP[m] / (TP[m] + FP[m])\n",
    "\n",
    "            if (TN[m] + FN[m] == 0) : precision0_macro[m] = 0\n",
    "            else :precision0_macro[m] = TN[m] / (TN[m] + FN[m])\n",
    "\n",
    "            if (TP[m] + FN[m] == 0) : recall1_macro[m] = 0\n",
    "            else :recall1_macro[m]    = TP[m] / (TP[m] + FN[m])\n",
    "\n",
    "            if (TN[m] + FP[m] == 0) : recall0_macro[m] = 0\n",
    "            recall0_macro[m]    = TN[m] / (TN[m] + FP[m])\n",
    " \n",
    "        macro_accuracy    = numpy.mean(accuracy_macro)\n",
    "        macro_precision1  = numpy.mean(precision1_macro)\n",
    "        macro_precision0  = numpy.mean(precision0_macro)\n",
    "        macro_recall1     = numpy.mean(recall1_macro)\n",
    "        macro_recall0     = numpy.mean(recall0_macro)\n",
    "\n",
    "        #F1_score还是按这个公式来算，用macro-P和macro-R\n",
    "        if (macro_precision1 + macro_recall1 == 0): macro_F1_score1 = 0\n",
    "        else: macro_F1_score1   = 2 * macro_precision1 * macro_recall1 / (macro_precision1 + macro_recall1)\n",
    "\n",
    "        if (macro_precision0 + macro_recall0 == 0): macro_F1_score0 = 0\n",
    "        else: macro_F1_score0   = 2 * macro_precision0 * macro_recall0 / (macro_precision0 + macro_recall0)\n",
    "            \n",
    " \n",
    "        #micro度量，是用TP、TN、FP、FN的均值来计算评价指标\n",
    "        TPM = numpy.mean(TP)\n",
    "        TNM = numpy.mean(TN)\n",
    "        FPM = numpy.mean(FP)\n",
    "        FNM = numpy.mean(FN)\n",
    "\n",
    "        micro_accuracy    = (TPM + TNM) / (TPM + FPM + FNM + TNM)\n",
    "\n",
    "        if(TPM + FPM ==0): micro_precision1  = 0#预防一些分母为0的情况\n",
    "        else: micro_precision1  = TPM / (TPM + FPM)\n",
    "\n",
    "        if(TNM + FNM ==0): micro_precision0  = 0\n",
    "        else: micro_precision0  = TNM / (TNM + FNM)\n",
    "\n",
    "        if (TPM + FNM == 0):micro_recall1 = 0\n",
    "        else: micro_recall1     = TPM / (TPM + FNM)\n",
    "\n",
    "        if (TNM + FPM == 0):micro_recall0 = 0\n",
    "        else: micro_recall0     = TNM / (TNM + FPM)\n",
    "\n",
    "        # F1_score仍然按这个公式来算，用micro-P和micro-R\n",
    "        if (micro_precision1 + micro_recall1 == 0): micro_F1_score1 = 0\n",
    "        else :micro_F1_score1   = 2 * micro_precision1 * micro_recall1 / (micro_precision1 + micro_recall1)\n",
    "\n",
    "        if (micro_precision0 + micro_recall0 == 0): micro_F1_score0 = 0\n",
    "        else :micro_F1_score0   = 2 * micro_precision0 * micro_recall0 / (micro_precision0 + micro_recall0)\n",
    "\n",
    "        print (\"*****************************macro*****************************\")\n",
    "        print (\"accuracy\",\":%.3f\" % macro_accuracy)\n",
    "        print (\"%20s\"%'precision',\"%12s\"%'recall',\"%12s\"%'F1_score')\n",
    "        print (\"%5s\" % \"0\", \"%14.3f\" % macro_precision0, \"%12.3f\" % macro_recall0, \"%12.3f\" %macro_F1_score0)\n",
    "        print (\"%5s\" % \"1\", \"%14.3f\" % macro_precision1, \"%12.3f\" % macro_recall1, \"%12.3f\" %macro_F1_score1)\n",
    "        print (\"%5s\" % \"avg\",\"%14.3f\" % ((macro_precision0+macro_precision1)/2), \\\n",
    "              \"%12.3f\" % ((macro_recall0+macro_recall1)/2), \"%12.3f\" %((macro_F1_score1+macro_F1_score0)/2))\n",
    "        print (\"*****************************micro*****************************\")\n",
    "        print (\"accuracy\",\":%.3f\" % micro_accuracy)\n",
    "        print (\"%20s\"%'precision',\"%12s\"%'recall',\"%12s\"%'F1_score')\n",
    "        print (\"%5s\" % \"0\", \"%14.3f\" % micro_precision0, \"%12.3f\" % micro_recall0, \"%12.3f\" %micro_F1_score0)\n",
    "        print (\"%5s\" % \"1\", \"%14.3f\" % micro_precision1, \"%12.3f\" % micro_recall1, \"%12.3f\" %micro_F1_score1)\n",
    "        print (\"%5s\" % \"avg\", \"%14.3f\" % ((micro_precision0 + micro_precision1) / 2), \\\n",
    "            \"%12.3f\" % ((micro_recall0 + micro_recall1) / 2), \"%12.3f\" % ((micro_F1_score0 + micro_F1_score1) / 2))\n",
    "\n",
    "        #return micro_F1_score1,micro_F1_score0\n",
    "        #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model F1 Score: 0.7611518915866743\n",
      "Model Accuracy: 0.8147985989492119\n",
      "Model Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85      1318\n",
      "           1       0.84      0.70      0.76       966\n",
      "\n",
      "    accuracy                           0.81      2284\n",
      "   macro avg       0.82      0.80      0.80      2284\n",
      "weighted avg       0.82      0.81      0.81      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# e-2: Train the Bernoulli_NaiveBayes classifier on the train set, and report its mean F 1-score on the dev set.\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "x_train = train_bow_vectors\n",
    "y_train = np.array(y_train)\n",
    "x_test = test_bow_vectors\n",
    "\n",
    "BernoulliNB = Bernoulli_NaiveBayes()\n",
    "BernoulliNB.train(x_train,y_train)\n",
    "y_pred = BernoulliNB.predict(x_test)\n",
    "\n",
    "print('Model F1 Score:', metrics.f1_score(y_test, y_pred))\n",
    "print('Model Accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
    "print('Model Classification Report: \\n', metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "#print(y_pred[:50])\n",
    "#print('All predicted labels: \\n', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f18cad820c14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBernoulliNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-0f9ab9777ab6>\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(self, true, predict)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#样本数都不等，显然是有错误的\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"真实结果与预测结果样本数不一致。\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "print (BernoulliNB.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e5b224a23404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "print (list(y_test))\n",
    "print (len(list(y_test)[0]))\n",
    "print (len(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
