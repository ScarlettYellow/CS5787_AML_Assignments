{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"input/split_train.csv\")\n",
    "x_train=train.drop('target',axis=1)#dataframe with index\n",
    "y_train=train.target\n",
    "\n",
    "test=pd.read_csv(\"input/dev.csv\")\n",
    "x_test=test.drop('target',axis=1)#dataframe with index\n",
    "y_test=test.target\n",
    "\n",
    "tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True) # word segmentation\n",
    "stemmer=PorterStemmer() # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre=[]\n",
    "test_pre=[]\n",
    "def preprocess(t,kpc):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    tee=url.sub(r'',t)\n",
    "    tee=re.sub('[^a-zA-Z]',\" \",tee)\n",
    "    tee=tee.lower()\n",
    "    res=tweettoken.tokenize(tee)\n",
    "    for i in res:\n",
    "        if i in stopwords.words('english'):\n",
    "            res.remove(i)\n",
    "    rest=[]\n",
    "    for k in res:\n",
    "        rest.append(stemmer.stem(k))\n",
    "    ret=\" \".join(rest)\n",
    "    if kpc==1:\n",
    "        train_pre.append(ret)\n",
    "    elif kpc==0:\n",
    "        test_pre.append(ret)\n",
    "\n",
    "def splitpro(t,q,m):\n",
    "         for j in range(q):\n",
    "                 preprocess(t[\"text\"].iloc[j],m)\n",
    "                 \n",
    "splitpro(x_train,5329,1)\n",
    "splitpro(x_test,2284,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "['aa', 'aba', 'abandon', 'abbswinston', 'abc', 'abcnew', 'abil', 'abl', 'ablaz', 'about', 'absolut', 'abstorm', 'abus', 'accept', 'access', 'accid', 'accident', 'accionempresa', 'accord', 'account', 'accus', 'achiev', 'acid', 'acr', 'across', 'act', 'action', 'activ', 'actual', 'ad', 'add', 'address', 'admit', 'adopt', 'adult', 'advanc', 'advisori', 'af', 'affect', 'affili', 'afghan', 'afghanistan', 'afp', 'afraid', 'africa', 'after', 'afternoon', 'aftershock', 'ag', 'again']\n",
      "[('ash', 169), ('australia', 192), ('collaps', 532), ('trent', 2721), ('bridg', 355), ('among', 99), ('worst', 2941), ('histori', 1240), ('england', 859), ('bundl', 381), ('great', 1140), ('michigan', 1652), ('techniqu', 2592), ('camp', 408), ('thank', 2617), ('hail', 1171), ('cnn', 523), ('tennesse', 2602), ('movi', 1714), ('theater', 2621), ('shoot', 2346), ('suspect', 2555), ('kill', 1428), ('polic', 1984), ('still', 2494), ('riot', 2205), ('coupl', 594), ('hour', 1264), ('left', 1483), ('to', 2665), ('up', 2788), ('class', 503), ('crack', 601), ('the', 2620), ('path', 1917), ('thi', 2631), ('morn', 1700), ('beach', 245), ('run', 2238), ('surfac', 2549), ('wound', 2945), ('right', 2203), ('knee', 1438), ('expert', 914), ('franc', 1042), ('begin', 262), ('examin', 897), ('airplan', 64), ('debri', 667), ('found', 1035)]\n",
      "[ 3  8  8 ...  6  6 24]\n",
      "(5329, 2991)\n",
      "total number of features in trainset: 2991\n"
     ]
    }
   ],
   "source": [
    "# d-1: use bag of words model on train set.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3,\n",
    "                             binary=True) #instantiate the CountVectorizer class, set M=3\n",
    "train_corpus = train_pre\n",
    "\n",
    "train_bow_vectors = vectorizer.fit_transform(train_corpus) #use fit() to create index, transform each document into a word frequency vector\n",
    "print(type(train_bow_vectors)) #type: sparse vector\n",
    "#print (train_bow_vectors)\n",
    "\n",
    "train_bow_vectors = train_bow_vectors.toarray()\n",
    "print (train_bow_vectors)\n",
    "print(type(train_bow_vectors)) #type: np array\n",
    "\n",
    "train_bow_voca_list = vectorizer.get_feature_names() #generate corpus into a vocabulary list\n",
    "train_bow_voca_dic = vectorizer.vocabulary_\n",
    "print(train_bow_voca_list[:50])\n",
    "print(list(train_bow_voca_dic.items())[:50])\n",
    "#print('vocabulary list of trainset:', train_bow_voca_list)\n",
    "#print( 'vocabulary dic of trainset:', train_bow_voca_dic)\n",
    "\n",
    "print(train_bow_vectors.sum(axis=0)) #count each word' frequency in the corpus\n",
    "\n",
    "X1 = train_bow_vectors.shape\n",
    "print (X1)\n",
    "train_bow_feature_num = X1[1] #vector length/number of features\n",
    "print ('total number of features in trainset:', train_bow_feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "['aa', 'aba', 'abandon', 'abbswinston', 'abc', 'abcnew', 'abil', 'abl', 'ablaz', 'about', 'absolut', 'abstorm', 'abus', 'accept', 'access', 'accid', 'accident', 'accionempresa', 'accord', 'account', 'accus', 'achiev', 'acid', 'acr', 'across', 'act', 'action', 'activ', 'actual', 'ad', 'add', 'address', 'admit', 'adopt', 'adult', 'advanc', 'advisori', 'af', 'affect', 'affili', 'afghan', 'afghanistan', 'afp', 'afraid', 'africa', 'after', 'afternoon', 'aftershock', 'ag', 'again']\n",
      "[('ash', 169), ('australia', 192), ('collaps', 532), ('trent', 2721), ('bridg', 355), ('among', 99), ('worst', 2941), ('histori', 1240), ('england', 859), ('bundl', 381), ('great', 1140), ('michigan', 1652), ('techniqu', 2592), ('camp', 408), ('thank', 2617), ('hail', 1171), ('cnn', 523), ('tennesse', 2602), ('movi', 1714), ('theater', 2621), ('shoot', 2346), ('suspect', 2555), ('kill', 1428), ('polic', 1984), ('still', 2494), ('riot', 2205), ('coupl', 594), ('hour', 1264), ('left', 1483), ('to', 2665), ('up', 2788), ('class', 503), ('crack', 601), ('the', 2620), ('path', 1917), ('thi', 2631), ('morn', 1700), ('beach', 245), ('run', 2238), ('surfac', 2549), ('wound', 2945), ('right', 2203), ('knee', 1438), ('expert', 914), ('franc', 1042), ('begin', 262), ('examin', 897), ('airplan', 64), ('debri', 667), ('found', 1035)]\n",
      "[ 0  6  6 ...  0  0 11]\n",
      "(2284, 2991)\n",
      "total number of features in dev set: 2991\n"
     ]
    }
   ],
   "source": [
    "# d-1: use bag of words model on dev set.\n",
    "\n",
    "test_corpus = test_pre\n",
    "\n",
    "test_bow_vectors = vectorizer.transform(test_corpus) #use the same set of tokens as trainset, transform each document into a word frequency vector\n",
    "print(type(test_bow_vectors)) #type: sparse vector\n",
    "#print (test_bow_vectors)\n",
    "\n",
    "test_bow_vectors = test_bow_vectors.toarray()\n",
    "print (test_bow_vectors)\n",
    "print(type(test_bow_vectors)) #type: np array\n",
    "\n",
    "test_bow_voca_list = vectorizer.get_feature_names() #generate corpus into a vocabulary list\n",
    "test_bow_voca_dic = vectorizer.vocabulary_\n",
    "print(test_bow_voca_list[:50])\n",
    "print(list(test_bow_voca_dic.items())[:50])\n",
    "#print('vocabulary list of dev set:', test_bow_voca_list)\n",
    "#print( 'vocabulary dic of dev set:', test_bow_voca_dic)\n",
    "\n",
    "print(test_bow_vectors.sum(axis=0)) #count each word' frequency in the corpus\n",
    "\n",
    "X2 = test_bow_vectors.shape\n",
    "print (X2)\n",
    "test_bow_feature_num = X2[1] #vector length/number of features\n",
    "print ('total number of features in dev set:', test_bow_feature_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
