{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"input/split_train.csv\")\n",
    "x_train=train.drop('target',axis=1)#dataframe with index\n",
    "y_train=train.target\n",
    "\n",
    "test=pd.read_csv(\"input/dev.csv\")\n",
    "x_test=test.drop('target',axis=1)#dataframe with index\n",
    "y_test=test.target\n",
    "\n",
    "tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True) # word segmentation\n",
    "stemmer=PorterStemmer() # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre=[]\n",
    "test_pre=[]\n",
    "def preprocess(t,kpc):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    tee=url.sub(r'',t)\n",
    "    tee=re.sub('[^a-zA-Z]',\" \",tee)\n",
    "    tee=tee.lower()\n",
    "    res=tweettoken.tokenize(tee)\n",
    "    for i in res:\n",
    "        if i in stopwords.words('english'):\n",
    "            res.remove(i)\n",
    "    rest=[]\n",
    "    for k in res:\n",
    "        rest.append(stemmer.stem(k))\n",
    "    ret=\" \".join(rest)\n",
    "    if kpc==1:\n",
    "        train_pre.append(ret)\n",
    "    elif kpc==0:\n",
    "        test_pre.append(ret)\n",
    "\n",
    "def splitpro(t,q,m):\n",
    "         for j in range(q):\n",
    "                 preprocess(t[\"text\"].iloc[j],m)\n",
    "                 \n",
    "splitpro(x_train,5329,1)\n",
    "splitpro(x_test,2284,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn-BernoulliNB to predict\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "\n",
    "x_train = train_bow_vectors\n",
    "y_train = np.array(y_train)\n",
    "x_test = test_bow_vectors\n",
    "\n",
    "bnb_sk = BernoulliNB()\n",
    "y_pred = bnb_sk.fit(x_train, y_train).predict(x_test)\n",
    "print(y_pred)\n",
    "\n",
    "print('Model F1 Score：\\n',metrics.f1_score(y_test, y_pred))\n",
    "print('Model Accuracy：\\n',metrics.accuracy_score(y_test, y_pred))\n",
    "print('Model Classification Report：\\n',metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
