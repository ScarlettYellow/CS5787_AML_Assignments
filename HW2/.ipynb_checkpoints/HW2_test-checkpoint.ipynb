{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"input/split_train.csv\")\n",
    "x_train=train.drop('target',axis=1)#dataframe with index\n",
    "y_train=train.target\n",
    "\n",
    "test=pd.read_csv(\"input/dev.csv\")\n",
    "x_test=test.drop('target',axis=1)#dataframe with index\n",
    "y_test=test.target\n",
    "\n",
    "tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True) # word segmentation\n",
    "stemmer=PorterStemmer() # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre=[]\n",
    "test_pre=[]\n",
    "def preprocess(t,kpc):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    tee=url.sub(r'',t)\n",
    "    tee=re.sub('[^a-zA-Z]',\" \",tee)\n",
    "    tee=tee.lower()\n",
    "    res=tweettoken.tokenize(tee)\n",
    "    for i in res:\n",
    "        if i in stopwords.words('english'):\n",
    "            res.remove(i)\n",
    "    rest=[]\n",
    "    for k in res:\n",
    "        rest.append(stemmer.stem(k))\n",
    "    ret=\" \".join(rest)\n",
    "    if kpc==1:\n",
    "        train_pre.append(ret)\n",
    "    elif kpc==0:\n",
    "        test_pre.append(ret)\n",
    "\n",
    "def splitpro(t,q,m):\n",
    "         for j in range(q):\n",
    "                 preprocess(t[\"text\"].iloc[j],m)\n",
    "                 \n",
    "splitpro(x_train,5329,1)\n",
    "splitpro(x_test,2284,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7463175122749591\n"
     ]
    }
   ],
   "source": [
    "#LR model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(train_ng_vectors,y_train)\n",
    "predict_lr=LR.predict(test_ng_vectors)\n",
    "print(f1_score(y_test,predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
